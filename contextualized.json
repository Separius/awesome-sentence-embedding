[
  {
    "paper_title": "Learned in Translation: Contextualized Word Vectors",
    "paper_link": "https://arxiv.org/abs/1708.00107",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/salesforce/cove"
      },
      {
        "language": "Keras",
        "link": "https://github.com/rgsachin/CoVe",
        "unofficial": true
      }
    ],
    "model_name": "CoVe",
    "pretrained_models": [
      {
        "name": "CoVe",
        "link": "https://github.com/salesforce/cove"
      }
    ]
  },
  {
    "paper_title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
    "paper_link": "https://arxiv.org/abs/2003.10555",
    "code": [
      {
        "language": "TF",
        "link": "https://github.com/google-research/electra"
      }
    ],
    "model_name": "ELECTRA",
    "pretrained_models": [
      {
        "name": "ELECTRA-Small",
        "link": "https://storage.googleapis.com/electra-data/electra_small.zip"
      },
      {
        "name": "ELECTRA-Base",
        "link": "https://storage.googleapis.com/electra-data/electra_base.zip"
      },
      {
        "name": "ELECTRA-Large",
        "link": "https://storage.googleapis.com/electra-data/electra_large.zip"
      }
    ]
  },
  {
    "paper_title": "Unsupervised Cross-lingual Representation Learning at Scale",
    "paper_link": "https://arxiv.org/abs/1911.02116",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/facebookresearch/xlm"
      }
    ],
    "model_name": "XLM-R (XLM-RoBERTa)",
    "pretrained_models": [
      {
        "name": "xlmr.large",
        "link": "https://dl.fbaipublicfiles.com/fairseq/models/xlmr.large.tar.gz"
      },
      {
        "name": "xlmr.base",
        "link": "https://dl.fbaipublicfiles.com/fairseq/models/xlmr.base.tar.gz"
      }
    ]
  },
  {
    "paper_title": "Universal Language Model Fine-tuning for Text Classification",
    "paper_link": "https://arxiv.org/abs/1801.06146",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/fastai/fastai/tree/ulmfit_v1",
        "unofficial": true
      }
    ],
    "model_name": "ULMFit",
    "pretrained_models": [
      {
        "name": "English",
        "link": "https://docs.fast.ai/text.html#Fine-tuning-a-language-model"
      },
      {
        "name": "Zoo",
        "link": "https://forums.fast.ai/t/language-model-zoo-gorilla/14623/1"
      }
    ]
  },
  {
    "paper_title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
    "paper_link": "https://arxiv.org/abs/1910.01108",
    "code": [
      {
        "language": "Pytorch, TF2.0",
        "link": "https://github.com/huggingface/transformers"
      }
    ],
    "model_name": "DistilBERT",
    "pretrained_models": [
      {
        "name": "DistilBERT",
        "link": "https://github.com/huggingface/transformers/tree/master/examples/distillation"
      }
    ]
  },
  {
    "paper_title": "MultiFiT: Efficient Multi-lingual Language Model Fine-tuning",
    "paper_link": "https://arxiv.org/abs/1909.04761",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/n-waves/ulmfit-multilingual"
      }
    ],
    "model_name": "MultiFiT"
  },
  {
    "paper_title": "Efficient Contextual Representation Learning Without Softmax Layer",
    "paper_link": "https://arxiv.org/abs/1902.11269",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/uclanlp/ELMO-C"
      }
    ],
    "model_name": "ELMO-C"
  },
  {
    "paper_title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
    "paper_link": "https://arxiv.org/abs/1909.11942",
    "code": [
      {
        "language": "TF",
        "link": "https://github.com/brightmart/albert_zh",
        "unofficial": true
      }
    ],
    "model_name": "ALBERT"
  },
  {
    "paper_title": "Extreme Language Model Compression with Optimal Subwords and Shared Projections",
    "paper_link": "https://arxiv.org/abs/1909.11687",
    "code": [
    ],
    "model_name": "BERT_DISTILLED"
  },
  {
    "paper_title": "UNITER: Learning UNiversal Image-TExt Representations",
    "paper_link": "https://arxiv.org/abs/1909.11740",
    "code": [
    ],
    "model_name": "UNITER"
  },
  {
    "paper_title": "MULE: Multimodal Universal Language Embedding",
    "paper_link": "https://arxiv.org/abs/1909.03493",
    "code": [
    ],
    "model_name": "MULE"
  },
  {
    "paper_title": "Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks",
    "paper_link": "https://arxiv.org/abs/1909.00964",
    "code": [
    ],
    "model_name": "Unicoder"
  },
  {
    "paper_title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization",
    "paper_link": "https://arxiv.org/abs/1905.06566",
    "code": [
    ],
    "model_name": "HIBERT"
  },
  {
    "paper_title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
    "paper_link": "https://arxiv.org/abs/1905.03197",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/microsoft/unilm"
      }
    ],
    "model_name": "UniLM",
    "pretrained_models": [
      {
        "name": "UniLMv1",
        "link": "https://github.com/microsoft/unilm#pre-trained-models"
      }
    ]
  },
  {
    "paper_title": "CamemBERT: a Tasty French Language Model",
    "paper_link": "https://arxiv.org/abs/1911.03894",
    "model_name": "CamemBERT",
    "pretrained_models": [
      {
        "name": "CamemBERT",
        "link": "https://camembert-model.fr/#download"
      }
    ]
  },
  {
    "paper_title": "Knowledge Enhanced Contextual Word Representations",
    "paper_link": "https://arxiv.org/abs/1909.04164",
    "code": [
    ],
    "model_name": "KnowBert"
  },
  {
    "paper_title": "TinyBERT: Distilling BERT for Natural Language Understanding",
    "paper_link": "https://arxiv.org/abs/1909.10351",
    "code": [
    ],
    "model_name": "TinyBERT"
  },
  {
    "paper_title": "K-BERT: Enabling Language Representation with Knowledge Graph",
    "paper_link": "https://arxiv.org/abs/1909.07606",
    "code": [
    ],
    "model_name": "K-BERT"
  },
  {
    "paper_title": "Subword ELMo",
    "paper_link": "https://arxiv.org/abs/1909.08357",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/Jiangtong-Li/Subword-ELMo/"
      }
    ]
  },
  {
    "paper_title": "ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations",
    "paper_link": "https://arxiv.org/abs/1911.00720",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/sinovation/ZEN"
      }
    ]
  },
  {
    "paper_title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans",
    "paper_link": "https://arxiv.org/abs/1907.10529",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/facebookresearch/SpanBERT"
      }
    ],
    "model_name": "SpanBERT",
    "pretrained_models": [
      {
        "name": "SpanBERT",
        "link": "https://github.com/facebookresearch/SpanBERT#pre-trained-models"
      }
    ]
  },
  {
    "paper_title": "Deep contextualized word representations",
    "paper_link": "https://arxiv.org/abs/1802.05365",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/allenai/allennlp"
      },
      {
        "language": "TF",
        "link": "https://github.com/allenai/bilm-tf"
      }
    ],
    "model_name": "ELMO",
    "pretrained_models": [
      {
        "name": "AllenNLP",
        "link": "https://allennlp.org/elmo"
      },
      {
        "name": "TF-Hub",
        "link": "https://tfhub.dev/google/elmo/2"
      }
    ]
  },
  {
    "paper_title": "Improving Language Understanding by Generative Pre-Training",
    "paper_link": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf",
    "code": [
      {
        "language": "TF",
        "link": "https://github.com/openai/finetune-transformer-lm"
      },
      {
        "language": "Keras",
        "link": "https://github.com/Separius/BERT-keras",
        "unofficial": true
      },
      {
        "language": "Pytorch, TF2.0",
        "link": "https://github.com/huggingface/transformers",
        "unofficial": true
      }
    ],
    "model_name": "GPT",
    "pretrained_models": [
      {
        "name": "GPT",
        "link": "https://github.com/openai/finetune-transformer-lm"
      }
    ],
    "s2_paper_id": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"
  },
  {
    "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "paper_link": "https://arxiv.org/abs/1810.04805",
    "code": [
      {
        "language": "TF",
        "link": "https://github.com/google-research/bert"
      },
      {
        "language": "Keras",
        "link": "https://github.com/Separius/BERT-keras",
        "unofficial": true
      },
      {
        "language": "Pytorch, TF2.0",
        "link": "https://github.com/huggingface/transformers",
        "unofficial": true
      },
      {
        "language": "MXNet",
        "link": "https://github.com/imgarylai/bert-embedding",
        "unofficial": true
      },
      {
        "language": "PaddlePaddle",
        "link": "https://github.com/PaddlePaddle/ERNIE",
        "unofficial": true
      },
      {
        "language": "TF",
        "link": "https://github.com/hanxiao/bert-as-service/",
        "unofficial": true
      },
      {
        "language": "Keras",
        "link": "https://github.com/CyberZHG/keras-bert",
        "unofficial": true
      }
    ],
    "model_name": "BERT",
    "pretrained_models": [
      {
        "name": "BERT",
        "link": "https://github.com/google-research/bert#pre-trained-models"
      },
      {
        "name": "ERNIE",
        "link": "https://github.com/PaddlePaddle/ERNIE"
      },
      {
        "name": "KoBERT",
        "link": "https://github.com/SKTBrain/KoBERT"
      }
    ]
  },
  {
    "paper_title": "Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation",
    "paper_link": "https://arxiv.org/abs/1807.03121",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/HIT-SCIR/ELMoForManyLangs"
      }
    ],
    "model_name": "ELMo",
    "pretrained_models": [
      {
        "name": "ELMo",
        "link": "https://github.com/HIT-SCIR/ELMoForManyLangs#downloads"
      }
    ]
  },
  {
    "paper_title": "Contextual String Embeddings for Sequence Labeling",
    "paper_link": "",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/zalandoresearch/flair"
      }
    ],
    "model_name": "Flair",
    "pretrained_models": [
      {
        "name": "Flair",
        "link": "https://github.com/zalandoresearch/flair/blob/master/flair/embeddings.py#L407"
      }
    ],
    "s2_paper_id": "421fc2556836a6b441de806d7b393a35b6eaea58"
  },
  {
    "paper_title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
    "paper_link": "https://arxiv.org/abs/1901.02860",
    "code": [
      {
        "language": "TF",
        "link": "https://github.com/kimiyoung/transformer-xl/tree/master/tf"
      },
      {
        "language": "Pytorch",
        "link": "https://github.com/kimiyoung/transformer-xl/tree/master/pytorch"
      },
      {
        "language": "Pytorch, TF2.0",
        "link": "https://github.com/huggingface/transformers",
        "unofficial": true
      }
    ],
    "model_name": "Transformer-XL",
    "pretrained_models": [
      {
        "name": "Transformer-XL",
        "link": "https://github.com/kimiyoung/transformer-xl/tree/master/tf"
      }
    ]
  },
  {
    "paper_title": "SciBERT: Pretrained Contextualized Embeddings for Scientific Text",
    "paper_link": "https://arxiv.org/abs/1903.10676",
    "code": [
      {
        "language": "Pytorch, TF",
        "link": "https://github.com/allenai/scibert"
      }
    ],
    "model_name": "SciBERT",
    "pretrained_models": [
      {
        "name": "SciBERT",
        "link": "https://github.com/allenai/scibert#downloading-trained-models"
      }
    ]
  },
  {
    "paper_title": "Pre-Training with Whole Word Masking for Chinese BERT",
    "paper_link": "https://arxiv.org/abs/1906.08101",
    "code": [
      {
        "language": "Pytorch, TF",
        "link": "https://github.com/ymcui/Chinese-BERT-wwm"
      }
    ],
    "model_name": "BERT-wwm",
    "pretrained_models": [
      {
        "name": "BERT-wwm",
        "link": "https://github.com/ymcui/Chinese-BERT-wwm#pytorch%E7%89%88%E6%9C%AC%E8%AF%B7%E4%BD%BF%E7%94%A8-%E7%9A%84pytorch-bert--06%E5%85%B6%E4%BB%96%E7%89%88%E6%9C%AC%E8%AF%B7%E8%87%AA%E8%A1%8C%E8%BD%AC%E6%8D%A2"
      }
    ]
  },
  {
    "paper_title": "BioBERT: pre-trained biomedical language representation model for biomedical text mining",
    "paper_link": "https://arxiv.org/abs/1901.08746",
    "code": [
      {
        "language": "TF",
        "link": "https://github.com/dmis-lab/biobert"
      }
    ],
    "model_name": "BioBERT",
    "pretrained_models": [
      {
        "name": "BioBERT",
        "link": "https://github.com/naver/biobert-pretrained"
      }
    ]
  },
  {
    "paper_title": "Cross-lingual Language Model Pretraining",
    "paper_link": "https://arxiv.org/abs/1901.07291",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/facebookresearch/XLM"
      },
      {
        "language": "Pytorch, TF2.0",
        "link": "https://github.com/huggingface/transformers",
        "unofficial": true
      }
    ],
    "model_name": "XLM",
    "pretrained_models": [
      {
        "name": "XLM",
        "link": "https://github.com/facebookresearch/XLM#pretrained-models"
      }
    ]
  },
  {
    "paper_title": "Direct Output Connection for a High-Rank Language Model",
    "paper_link": "https://arxiv.org/abs/1808.10143",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/nttcslab-nlp/doc_lm"
      }
    ],
    "model_name": "DOC",
    "pretrained_models": [
      {
        "name": "DOC",
        "link": "https://drive.google.com/open?id=1ug-6ISrXHEGcWTk5KIw8Ojdjuww-i-Ci"
      }
    ]
  },
  {
    "paper_title": "Language Models are Unsupervised Multitask Learners",
    "paper_link": "https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf",
    "code": [
      {
        "language": "TF",
        "link": "https://github.com/openai/gpt-2"
      },
      {
        "language": "Pytorch, TF2.0",
        "link": "https://github.com/huggingface/transformers",
        "unofficial": true
      },
      {
        "language": "Keras",
        "link": "https://github.com/CyberZHG/keras-gpt-2",
        "unofficial": true
      }
    ],
    "model_name": "GPT-2",
    "pretrained_models": [
      {
        "name": "117M",
        "link": "https://github.com/openai/gpt-2"
      }
    ]
  },
  {
    "paper_title": "Efficient Contextualized Representation:Language Model Pruning for Sequence Labeling",
    "paper_link": "https://arxiv.org/abs/1804.07827",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/LiyuanLucasLiu/LD-Net"
      }
    ],
    "model_name": "LD-Net",
    "pretrained_models": [
      {
        "name": "LD-Net",
        "link": "https://github.com/LiyuanLucasLiu/LD-Net#language-models"
      }
    ]
  },
  {
    "paper_title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding",
    "paper_link": "https://arxiv.org/abs/1907.12412",
    "code": [
      {
        "language": "PaddlePaddle",
        "link": "https://github.com/PaddlePaddle/ERNIE"
      }
    ],
    "model_name": "ERNIE 2.0",
    "pretrained_models": [
      {
        "name": "ERNIE 2.0 Large",
        "link": "https://github.com/PaddlePaddle/ERNIE#models"
      }
    ]
  },
  {
    "paper_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
    "paper_link": "https://arxiv.org/abs/1907.11692",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/pytorch/fairseq"
      },
      {
        "language": "Pytorch, TF2.0",
        "link": "https://github.com/huggingface/transformers",
        "unofficial": true
      }
    ],
    "model_name": "RoBERTa",
    "pretrained_models": [
      {
        "name": "roberta.large",
        "link": "https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md#pre-trained-models"
      }
    ]
  },
  {
    "paper_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
    "paper_link": "https://arxiv.org/abs/1906.08237",
    "code": [
      {
        "language": "TF",
        "link": "https://github.com/zihangdai/xlnet"
      },
      {
        "language": "Pytorch, TF2.0",
        "link": "https://github.com/huggingface/transformers",
        "unofficial": true
      }
    ],
    "model_name": "XLNet",
    "pretrained_models": [
      {
        "name": "XLNet-Large, Cased",
        "link": "https://github.com/zihangdai/xlnet#released-models"
      }
    ]
  },
  {
    "paper_title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
    "paper_link": "https://arxiv.org/abs/1901.11504",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/namisan/mt-dnn"
      }
    ],
    "model_name": "MT-DNN",
    "pretrained_models": [
      {
        "name": "MT-DNN",
        "link": "https://github.com/namisan/mt-dnn/blob/master/download.sh"
      }
    ]
  },
  {
    "paper_title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission",
    "paper_link": "https://arxiv.org/abs/1904.05342",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/kexinhuang12345/clinicalBERT"
      }
    ],
    "model_name": "ClinicalBERT",
    "pretrained_models": [
      {
        "name": "ClinicalBERT",
        "link": "https://drive.google.com/file/d/1t8L9w-r88Q5-sfC993x2Tjt1pu--A900/view"
      }
    ]
  },
  {
    "paper_title": "Publicly Available Clinical BERT Embeddings",
    "paper_link": "https://arxiv.org/abs/1904.03323",
    "code": [
      {
        "language": "Text",
        "link": "https://github.com/EmilyAlsentzer/clinicalBERT"
      }
    ],
    "model_name": "clinicalBERT",
    "pretrained_models": [
      {
        "name": "clinicalBERT",
        "link": "https://www.dropbox.com/s/8armk04fu16algz/pretrained_bert_tf.tar.gz?dl=0"
      }
    ]
  },
  {
    "paper_title": "ERNIE: Enhanced Language Representation with Informative Entities",
    "paper_link": "https://arxiv.org/abs/1905.07129",
    "code": [
      {
        "language": "Pytorch",
        "link": "https://github.com/thunlp/ERNIE"
      }
    ],
    "model_name": "ERNIE",
    "pretrained_models": [
      {
        "name": "ERNIE",
        "link": "https://drive.google.com/open?id=1m673-YB-4j1ISNDlk5oZjpPF2El7vn6f"
      }
    ]
  },
  {
    "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "paper_link": "https://arxiv.org/abs/1910.10683",
    "code": [
      {
        "language": "TF",
        "link": "https://github.com/google-research/text-to-text-transfer-transformer"
      }
    ],
    "model_name": "T5",
    "pretrained_models": [
      {
        "name": "T5",
        "link": "https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints"
      }
    ]
  }
]
